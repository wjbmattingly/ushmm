{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to the USHMM Package\n",
    "\n",
    "This notebook demonstrates the usage of the USHMM (United States Holocaust Memorial Museum) package, a suite of tools designed for working with data from the USHMM. The package provides functionalities to process and analyze testimonies and related documents.\n",
    "\n",
    "Key features of the USHMM package include:\n",
    "1. Converting PDF files to images\n",
    "2. Performing OCR (Optical Character Recognition) on images\n",
    "3. Cleaning and processing extracted text\n",
    "4. Removing footers from images\n",
    "5. Generating structured HTML output from processed texts\n",
    "\n",
    "In the following cells, we'll walk through each step of the process using the USHMM package, from converting a PDF testimony to a final, cleaned HTML output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing USHMM Functions\n",
    "\n",
    "In this section, we'll import the necessary functions from the USHMM package and briefly explain their purposes:\n",
    "\n",
    "1. `pdf_to_images`: Converts PDF files to images\n",
    "2. `images_to_text`: Performs OCR on images to extract text\n",
    "3. `clean_texts`: Processes and cleans the extracted text\n",
    "4. `remove_footers`: Removes footers from images\n",
    "5. `process_testimony_texts`: Generates structured HTML output from processed texts\n",
    "\n",
    "These functions form the core of our testimony processing pipeline, allowing us to transform PDF testimonies into clean, structured HTML documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ushmm import pdf_to_images, images_to_text, clean_texts, remove_footers, process_testimony_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow\n",
    "\n",
    "In this section, we'll walk through the step-by-step process of using the USHMM package to convert a PDF testimony into a structured HTML document. Each step corresponds to a function we imported earlier:\n",
    "\n",
    "1. Convert PDF to Images: We'll use `pdf_to_images` to convert our PDF file into a series of image files.\n",
    "2. Remove Footers: The `remove_footers` function will be used to crop out any footer information from our images.\n",
    "3. Perform OCR: We'll use `images_to_text` to extract text from our cropped images.\n",
    "4. Clean Extracted Text: The `clean_texts` function will process and clean the extracted text.\n",
    "5. Generate HTML: Finally, we'll use `process_testimony_texts` to create a structured HTML document from our cleaned text.\n",
    "\n",
    "Let's go through each of these steps in detail:\n",
    "\n",
    "\n",
    "### Step 1: Convert PDF to Images\n",
    "\n",
    "In this step, we use the `pdf_to_images` function to convert our PDF testimony into a series of image files. This is the first crucial step in our processing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: data/RG-50.030.0047_trs_en/images/0001.jpg\n",
      "Saved: data/RG-50.030.0047_trs_en/images/0002.jpg\n",
      "Saved: data/RG-50.030.0047_trs_en/images/0003.jpg\n",
      "Saved: data/RG-50.030.0047_trs_en/images/0004.jpg\n",
      "Saved: data/RG-50.030.0047_trs_en/images/0005.jpg\n",
      "Saved: data/RG-50.030.0047_trs_en/images/0006.jpg\n",
      "Saved: data/RG-50.030.0047_trs_en/images/0007.jpg\n",
      "Saved: data/RG-50.030.0047_trs_en/images/0008.jpg\n",
      "Saved: data/RG-50.030.0047_trs_en/images/0009.jpg\n",
      "Saved: data/RG-50.030.0047_trs_en/images/0010.jpg\n",
      "Saved: data/RG-50.030.0047_trs_en/images/0011.jpg\n",
      "Saved: data/RG-50.030.0047_trs_en/images/0012.jpg\n",
      "Saved: data/RG-50.030.0047_trs_en/images/0013.jpg\n",
      "Saved: data/RG-50.030.0047_trs_en/images/0014.jpg\n",
      "Saved: data/RG-50.030.0047_trs_en/images/0015.jpg\n",
      "Saved: data/RG-50.030.0047_trs_en/images/0016.jpg\n",
      "Saved: data/RG-50.030.0047_trs_en/images/0017.jpg\n",
      "Saved: data/RG-50.030.0047_trs_en/images/0018.jpg\n",
      "Saved: data/RG-50.030.0047_trs_en/images/0019.jpg\n"
     ]
    }
   ],
   "source": [
    "images = pdf_to_images(\"data/RG-50.030.0047_trs_en.pdf\", \"data/RG-50.030.0047_trs_en/images\", save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Remove Footers\n",
    "\n",
    "In this step, we use the `remove_footers` function to crop out any footer information from our images. This is an important preprocessing step that helps improve the accuracy of the subsequent OCR process by removing potentially confusing or irrelevant text from the bottom of each page.\n",
    "\n",
    "The `remove_footers` function takes the directory containing our images as input, processes each image to remove the footer, and saves the cropped images to a new directory. This step ensures that we're focusing on the main content of each page for our text extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading: 0012.jpg\n",
      "image read\n",
      "Footer Found at 1946. Cropping Image\n",
      "Processed image saved at: data/RG-50.030.0047_trs_en/images_cropped/0012.jpg\n",
      "reading: 0006.jpg\n",
      "image read\n",
      "Processed image saved at: data/RG-50.030.0047_trs_en/images_cropped/0006.jpg\n",
      "reading: 0007.jpg\n",
      "image read\n",
      "Footer Found at 1946. Cropping Image\n",
      "Processed image saved at: data/RG-50.030.0047_trs_en/images_cropped/0007.jpg\n",
      "reading: 0013.jpg\n",
      "image read\n",
      "Processed image saved at: data/RG-50.030.0047_trs_en/images_cropped/0013.jpg\n",
      "reading: 0005.jpg\n",
      "image read\n",
      "Footer Found at 1907. Cropping Image\n",
      "Processed image saved at: data/RG-50.030.0047_trs_en/images_cropped/0005.jpg\n",
      "reading: 0011.jpg\n",
      "image read\n",
      "Footer Found at 1907. Cropping Image\n",
      "Processed image saved at: data/RG-50.030.0047_trs_en/images_cropped/0011.jpg\n",
      "reading: 0010.jpg\n",
      "image read\n",
      "Processed image saved at: data/RG-50.030.0047_trs_en/images_cropped/0010.jpg\n",
      "reading: 0004.jpg\n",
      "image read\n",
      "Footer Found at 1907. Cropping Image\n",
      "Processed image saved at: data/RG-50.030.0047_trs_en/images_cropped/0004.jpg\n",
      "reading: 0014.jpg\n",
      "image read\n",
      "Processed image saved at: data/RG-50.030.0047_trs_en/images_cropped/0014.jpg\n",
      "reading: 0015.jpg\n",
      "image read\n",
      "Processed image saved at: data/RG-50.030.0047_trs_en/images_cropped/0015.jpg\n",
      "reading: 0001.jpg\n",
      "image read\n",
      "Processed image saved at: data/RG-50.030.0047_trs_en/images_cropped/0001.jpg\n",
      "reading: 0017.jpg\n",
      "image read\n",
      "Processed image saved at: data/RG-50.030.0047_trs_en/images_cropped/0017.jpg\n",
      "reading: 0003.jpg\n",
      "image read\n",
      "Processed image saved at: data/RG-50.030.0047_trs_en/images_cropped/0003.jpg\n",
      "reading: 0002.jpg\n",
      "image read\n",
      "Processed image saved at: data/RG-50.030.0047_trs_en/images_cropped/0002.jpg\n",
      "reading: 0016.jpg\n",
      "image read\n",
      "Processed image saved at: data/RG-50.030.0047_trs_en/images_cropped/0016.jpg\n",
      "reading: 0018.jpg\n",
      "image read\n",
      "Footer Found at 1946. Cropping Image\n",
      "Processed image saved at: data/RG-50.030.0047_trs_en/images_cropped/0018.jpg\n",
      "reading: 0019.jpg\n",
      "image read\n",
      "Processed image saved at: data/RG-50.030.0047_trs_en/images_cropped/0019.jpg\n",
      "reading: 0009.jpg\n",
      "image read\n",
      "Processed image saved at: data/RG-50.030.0047_trs_en/images_cropped/0009.jpg\n",
      "reading: 0008.jpg\n",
      "image read\n",
      "Processed image saved at: data/RG-50.030.0047_trs_en/images_cropped/0008.jpg\n"
     ]
    }
   ],
   "source": [
    "cropped_images = remove_footers('data/RG-50.030.0047_trs_en/images', output_directory='data/RG-50.030.0047_trs_en/images_cropped', save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Perform OCR\n",
    "\n",
    "In this step, we use the `images_to_text` function to extract text from our cropped images. This process, known as Optical Character Recognition (OCR), converts the image data into machine-readable text. The function processes each image in the specified directory, performs OCR, and saves the extracted text. This step is crucial as it transforms our visual data into textual data that we can further process and analyze.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0012.jpg\n",
      "Saved: data/RG-50.030.0047_trs_en/text/0012.txt\n",
      "0006.jpg\n",
      "Saved: data/RG-50.030.0047_trs_en/text/0006.txt\n",
      "0007.jpg\n",
      "Saved: data/RG-50.030.0047_trs_en/text/0007.txt\n",
      "0013.jpg\n",
      "Saved: data/RG-50.030.0047_trs_en/text/0013.txt\n",
      "0005.jpg\n",
      "Saved: data/RG-50.030.0047_trs_en/text/0005.txt\n",
      "0011.jpg\n",
      "Saved: data/RG-50.030.0047_trs_en/text/0011.txt\n",
      "0010.jpg\n",
      "Saved: data/RG-50.030.0047_trs_en/text/0010.txt\n",
      "0004.jpg\n",
      "Saved: data/RG-50.030.0047_trs_en/text/0004.txt\n",
      "0014.jpg\n",
      "Saved: data/RG-50.030.0047_trs_en/text/0014.txt\n",
      "0015.jpg\n",
      "Saved: data/RG-50.030.0047_trs_en/text/0015.txt\n",
      "0001.jpg\n",
      "Saved: data/RG-50.030.0047_trs_en/text/0001.txt\n",
      "0017.jpg\n",
      "Saved: data/RG-50.030.0047_trs_en/text/0017.txt\n",
      "0003.jpg\n",
      "Saved: data/RG-50.030.0047_trs_en/text/0003.txt\n",
      "0002.jpg\n",
      "Saved: data/RG-50.030.0047_trs_en/text/0002.txt\n",
      "0016.jpg\n",
      "Saved: data/RG-50.030.0047_trs_en/text/0016.txt\n",
      "0018.jpg\n",
      "Saved: data/RG-50.030.0047_trs_en/text/0018.txt\n",
      "0019.jpg\n",
      "Saved: data/RG-50.030.0047_trs_en/text/0019.txt\n",
      "0009.jpg\n",
      "Saved: data/RG-50.030.0047_trs_en/text/0009.txt\n",
      "0008.jpg\n",
      "Saved: data/RG-50.030.0047_trs_en/text/0008.txt\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "texts = images_to_text(\"data/RG-50.030.0047_trs_en/images_cropped\", save=True, output_folder=\"data/RG-50.030.0047_trs_en/text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Clean Extracted Text\n",
    "\n",
    "In this step, we use the `clean_texts` function to process and clean the extracted text. This function takes the raw OCR output and applies various cleaning operations to improve the quality and consistency of the text. This may include removing extraneous whitespace, correcting common OCR errors, and standardizing formatting. The cleaned text is then saved to a new directory, preparing it for the final step of our process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0007.txt\n",
      "Header Found: USHMM Archives RG-50.030*0047 5\n",
      "Timestamp Found: 01:16:04\n",
      "Normalizing characters...\n",
      "Changed: ’ -> '\n",
      "Normalizing quotes...\n",
      "Changed: ' -> \"\n",
      "Normalized 1 instances of excessive line breaks.\n",
      "0013.txt\n",
      "Header Found: USHMM Archives RG-50.030*0047 1\n",
      "Timestamp Found: 01:41:26\n",
      "Timestamp Found: 01:39:00\n",
      "Normalizing characters...\n",
      "Changed: é -> e\n",
      "Changed: ” -> \"\n",
      "Normalizing quotes...\n",
      "Normalized 2 instances of excessive line breaks.\n",
      "0012.txt\n",
      "Header Found: USHMM Archives RG-50.030*0047 1\n",
      "Timestamp Found: 01:33:29\n",
      "Timestamp Found: 01:36:44\n",
      "Normalizing characters...\n",
      "Changed: ’ -> '\n",
      "Changed: “ -> \"\n",
      "Changed: ” -> \"\n",
      "Normalizing quotes...\n",
      "Changed: ' -> \"\n",
      "Normalized 2 instances of excessive line breaks.\n",
      "0006.txt\n",
      "Header Found: USHMM Archives RG-50.030*0047 4\n",
      "Timestamp Found: 01:11:52\n",
      "Timestamp Found: 01:13:36\n",
      "Normalizing characters...\n",
      "Normalizing quotes...\n",
      "Normalized 2 instances of excessive line breaks.\n",
      "0010.txt\n",
      "Header Found: USHMM Archives RG-50.030*0047 8\n",
      "Timestamp Found: 01:28:27\n",
      "Normalizing characters...\n",
      "Changed: é -> e\n",
      "Normalizing quotes...\n",
      "Normalized 1 instances of excessive line breaks.\n",
      "0004.txt\n",
      "Header Found: USHMM Archives RG-50.030*0047 2\n",
      "Timestamp Found: 01:06:14\n",
      "Normalizing characters...\n",
      "Changed: ” -> \"\n",
      "Normalizing quotes...\n",
      "Changed: ' -> \"\n",
      "Normalized 1 instances of excessive line breaks.\n",
      "0005.txt\n",
      "Header Found: USHMM Archives RG-50.030*0047 3\n",
      "Timestamp Found: 01:09:29\n",
      "Normalizing characters...\n",
      "Changed: ’ -> '\n",
      "Normalizing quotes...\n",
      "Changed: ' -> \"\n",
      "Normalized 1 instances of excessive line breaks.\n",
      "0011.txt\n",
      "Header Found: USHMM Archives RG-50.030*0047 9\n",
      "Timestamp Found: 01:31:26\n",
      "Timestamp Found: 01:32:58\n",
      "Normalizing characters...\n",
      "Changed: ’ -> '\n",
      "Changed: ° -> deg\n",
      "Changed: é -> e\n",
      "Changed: “ -> \"\n",
      "Changed: ” -> \"\n",
      "Changed: é -> e\n",
      "Normalizing quotes...\n",
      "Normalized 2 instances of excessive line breaks.\n",
      "0015.txt\n",
      "Header Found: USHMM Archives RG-50.030*0047 1\n",
      "Timestamp Found: 01:47:50\n",
      "Timestamp Found: 01:49:53\n",
      "Normalizing characters...\n",
      "Changed: “ -> \"\n",
      "Changed: ” -> \"\n",
      "Normalizing quotes...\n",
      "Normalized 1 instances of excessive line breaks.\n",
      "0001.txt\n",
      "Normalizing characters...\n",
      "Normalizing quotes...\n",
      "0014.txt\n",
      "Header Found: USHMM Archives RG-50.030*0047 1\n",
      "Timestamp Found: 01:43:47\n",
      "Timestamp Found: 01:46:22\n",
      "Normalizing characters...\n",
      "Changed: ” -> \"\n",
      "Normalizing quotes...\n",
      "Normalized 1 instances of excessive line breaks.\n",
      "0002.txt\n",
      "Normalizing characters...\n",
      "Changed: ’ -> '\n",
      "Normalizing quotes...\n",
      "0016.txt\n",
      "Header Found: USHMM Archives RG-50.030*0047 1\n",
      "Timestamp Found: 01:52:16\n",
      "Timestamp Found: 01:54:37\n",
      "Normalizing characters...\n",
      "Changed: “ -> \"\n",
      "Changed: ” -> \"\n",
      "Normalizing quotes...\n",
      "Normalized 2 instances of excessive line breaks.\n",
      "0017.txt\n",
      "Header Found: USHMM Archives RG-50.030*0047 1\n",
      "Timestamp Found: 01:58:00\n",
      "Normalizing characters...\n",
      "Normalizing quotes...\n",
      "Normalized 1 instances of excessive line breaks.\n",
      "0003.txt\n",
      "Timestamp Found: 01:00:20\n",
      "Timestamp Found: 01:03:23\n",
      "Normalizing characters...\n",
      "Changed: “ -> \"\n",
      "Changed: ” -> \"\n",
      "Changed: “ -> \"\n",
      "Changed: ” -> \"\n",
      "Changed: “ -> \"\n",
      "Changed: ” -> \"\n",
      "Normalizing quotes...\n",
      "Normalized 2 instances of excessive line breaks.\n",
      "0019.txt\n",
      "Header Found: USHMM Archives RG-50.030*0047 1\n",
      "Normalizing characters...\n",
      "Changed: ’ -> '\n",
      "Normalizing quotes...\n",
      "0018.txt\n",
      "Header Found: USHMM Archives RG-50.030*0047 1\n",
      "Timestamp Found: 02:00:34\n",
      "Normalizing characters...\n",
      "Changed: é -> e\n",
      "Changed: ® -> (r)\n",
      "Changed: é -> e\n",
      "Changed: ’ -> '\n",
      "Changed: é -> e\n",
      "Changed: ’ -> '\n",
      "Normalizing quotes...\n",
      "Normalized 1 instances of excessive line breaks.\n",
      "0008.txt\n",
      "Header Found: USHMM Archives RG-50.030*0047 6\n",
      "Timestamp Found: 01:18:14\n",
      "Timestamp Found: 01:20:46\n",
      "Normalizing characters...\n",
      "Changed: — -> --\n",
      "Changed: “ -> \"\n",
      "Changed: ” -> \"\n",
      "Changed: “ -> \"\n",
      "Normalizing quotes...\n",
      "Normalized 2 instances of excessive line breaks.\n",
      "0009.txt\n",
      "Header Found: USHMM Archives RG-50.030*0047 7\n",
      "Timestamp Found: 01:26:10\n",
      "Timestamp Found: 01:23:13\n",
      "Normalizing characters...\n",
      "Changed: ’ -> '\n",
      "Normalizing quotes...\n",
      "Normalized 1 instances of excessive line breaks.\n"
     ]
    }
   ],
   "source": [
    "texts = clean_texts(\"data/RG-50.030.0047_trs_en/text\", save=True, output_directory=\"data/RG-50.030.0047_trs_en/clean_text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Generate HTML\n",
    "\n",
    "Finally, we use the `process_testimony_texts` function to create a structured HTML document from our cleaned text. This function takes the cleaned text files, processes them to identify different elements of the testimony (such as speakers, questions, and responses), and formats them into an HTML structure. This step transforms our raw text data into a more readable and navigable format, making it easier to analyze and present the testimony."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_result = process_testimony_texts('data/RG-50.030.0047_trs_en/clean_text', output_file=\"data/RG-50.030.0047_trs_en/RG-50.030.0047_trs_en.html\", save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
